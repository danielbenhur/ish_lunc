#!/usr/bin/env python3
"""
aggregate_presentation.py (atualizado)

- Corrige criação de colunas duplicadas (_x/_y) ao fazer merge.
- Permite múltiplas agregações (--agg mean median max min ou --agg all).
- Permite agregar para múltiplos targets (colunas) via --targets (default: cs_ish).
  Use '--targets all' para agregar cs_ish + todas as colunas starting with 'ire_cs_'.
- Escreve UMA camada de saída 'agg_<presentation_basename>' contendo colunas
  <target>_<agg> (ex.: 'cs_ish_mean', 'ire_cs_amb_mean', etc).
"""
import os
import argparse
import geopandas as gpd
import pandas as pd
import numpy as np
import fiona

def _get_local_utm_crs(gdf):
    if gdf.crs is None:
        raise ValueError("Input GeoDataFrame has no CRS.")
    gdf4326 = gdf.to_crs(epsg=4326)
    centroid = gdf4326.unary_union.centroid
    lon = centroid.x
    lat = centroid.y
    zone = int((lon + 180) / 6) + 1
    south = lat < 0
    proj4 = f"+proj=utm +zone={zone} +datum=WGS84 +units=m +no_defs"
    if south:
        proj4 += " +south"
    return proj4

def _weighted_median(values, weights):
    # values, weights: 1D numpy arrays
    if len(values) == 0:
        return np.nan
    mask = ~np.isnan(values)
    values = values[mask]
    weights = weights[mask]
    if len(values) == 0:
        return np.nan
    weights = np.array(weights, dtype=float)
    if weights.sum() == 0:
        return float(np.nanmedian(values))
    sorter = np.argsort(values)
    values_sorted = values[sorter]
    weights_sorted = weights[sorter]
    cumsum = np.cumsum(weights_sorted)
    cutoff = weights_sorted.sum() / 2.0
    idx = np.searchsorted(cumsum, cutoff)
    return float(values_sorted[min(idx, len(values_sorted)-1)])

def _safe_write_layer_to_gpkg(gpkg_path, layer_name, gdf_to_write):
    gpkg_path = os.path.abspath(gpkg_path)
    if not os.path.exists(gpkg_path):
        gdf_to_write.to_file(gpkg_path, layer=layer_name, driver="GPKG")
        return

    layers = fiona.listlayers(gpkg_path)
    if layer_name not in layers:
        gdf_to_write.to_file(gpkg_path, layer=layer_name, driver="GPKG")
        return

    temp_path = gpkg_path + ".tmp.gpkg"
    if os.path.exists(temp_path):
        try:
            os.remove(temp_path)
        except Exception:
            pass

    for lyr in layers:
        if lyr == layer_name:
            continue
        gdf = gpd.read_file(gpkg_path, layer=lyr)
        if not os.path.exists(temp_path):
            gdf.to_file(temp_path, layer=lyr, driver="GPKG")
        else:
            gdf.to_file(temp_path, layer=lyr, driver="GPKG", mode="a")

    if not os.path.exists(temp_path):
        gdf_to_write.to_file(temp_path, layer=layer_name, driver="GPKG")
    else:
        gdf_to_write.to_file(temp_path, layer=layer_name, driver="GPKG", mode="a")

    os.replace(temp_path, gpkg_path)

SUPPORTED_AGGS = ("mean", "median", "max", "min")

def aggregate_presentation_gpkg(input_gpkg,
                                input_layer="regiao_completa",
                                presentation_gpkg=None,
                                presentation_layer=None,
                                id_field="id_apresent",
                                aggs=("mean",),
                                targets=("cs_ish",),
                                output_gpkg=None,
                                verbose=True):
    """
    input_gpkg: path to ish gpkg (generated by joinISH)
    presentation_gpkg: file with presentation polygons (e.g. municipalities)
    aggs: iterable of aggregation names (subset of SUPPORTED_AGGS) or ('mean',) by default.
    targets: iterable of target column names to aggregate (default ('cs_ish',)).
             Use 'all' to mean cs_ish plus all columns starting with 'ire_cs_' found in input layer.
    """
    if presentation_gpkg is None:
        raise ValueError("presentation_gpkg must be provided.")

    # normalize aggs
    if isinstance(aggs, str):
        aggs = (aggs,)
    aggs = list(aggs)
    if "all" in aggs:
        aggs = list(SUPPORTED_AGGS)
    for a in aggs:
        if a not in SUPPORTED_AGGS:
            raise ValueError(f"Unsupported aggregation '{a}'. Supported: {SUPPORTED_AGGS} or 'all'.")

    # normalize targets
    if isinstance(targets, str):
        targets = (targets,)
    targets = list(targets)

    if output_gpkg is None:
        output_gpkg = input_gpkg

    if verbose:
        print("Loading input GPKG:", input_gpkg, "layer:", input_layer)
    gdf_input = gpd.read_file(input_gpkg, layer=input_layer)

    # determine available targets that are sensible (cs_ish and ire_cs_*)
    available_targets = [c for c in gdf_input.columns if (c == "cs_ish" or c.startswith("ire_cs_"))]
    if verbose:
        print("Available targets in input:", available_targets)

    # interpret targets 'all'
    if "all" in targets:
        targets = available_targets.copy()

    # validate targets
    for t in targets:
        if t not in available_targets:
            raise ValueError(f"Requested target '{t}' not found in input layer. Available: {available_targets}")

    if verbose:
        print("Targets to aggregate:", targets)
        print("Aggregation methods:", aggs)

    # load presentation
    if presentation_layer is None:
        layers = fiona.listlayers(presentation_gpkg)
        if not layers:
            raise ValueError("No layers found in presentation_gpkg: " + str(presentation_gpkg))
        presentation_layer = layers[0]
    if verbose:
        print("Loading presentation:", presentation_gpkg, "layer:", presentation_layer)
    gdf_pres = gpd.read_file(presentation_gpkg, layer=presentation_layer)

    if id_field not in gdf_pres.columns:
        raise ValueError(f"id_field '{id_field}' not found in presentation layer columns: {gdf_pres.columns.tolist()}")

    # reproject presentation to input CRS if needed
    if gdf_input.crs != gdf_pres.crs:
        if verbose:
            print("Reprojecting presentation layer to input CRS:", gdf_input.crs)
        gdf_pres = gdf_pres.to_crs(gdf_input.crs)

    # choose projected CRS for area calc
    try:
        project_crs = _get_local_utm_crs(gdf_pres)
        if verbose:
            print("Using local projected CRS for area calculations:", project_crs)
    except Exception:
        if gdf_input.crs is not None and gdf_input.crs.is_projected:
            project_crs = gdf_input.crs
            if verbose:
                print("Fallback: using input CRS for area calc:", project_crs)
        else:
            project_crs = "EPSG:3857"
            if verbose:
                print("Fallback: using EPSG:3857 for area calc")

    # project to working CRS
    gdf_input_p = gdf_input.to_crs(project_crs)
    gdf_pres_p = gdf_pres.to_crs(project_crs)

    # compute presentation area in km2
    gdf_pres_p["area_apresent_km2"] = gdf_pres_p.geometry.area / 1e6

    if verbose:
        print("Computing intersections (this may take time)...")

    # select only needed columns from input (to keep memory down)
    input_select_cols = ["cobacia"] + targets
    input_select = gdf_input_p[input_select_cols + ["geometry"]].copy()

    inter = gpd.overlay(gdf_pres_p, input_select, how="intersection")

    layer_basename = os.path.splitext(os.path.basename(presentation_gpkg))[0]
    out_layer_name = f"agg_{layer_basename}"

    # copy presentation (projected) as result frame
    result_pres = gdf_pres_p.copy()

    # if no intersections, create empty agg columns and write
    if inter.empty:
        if verbose:
            print("No intersections found. Writing empty result layer.")
        for t in targets:
            for a in aggs:
                result_pres[f"{t}_{a}"] = np.nan
        result_pres_out = result_pres.to_crs(gdf_pres.crs)
        _safe_write_layer_to_gpkg(output_gpkg, out_layer_name, result_pres_out)
        if verbose:
            print("Written layer", out_layer_name, "to", output_gpkg)
        return output_gpkg

    # compute intersection piece area (km2)
    inter["area_inter_km2"] = inter.geometry.area / 1e6

    # ensure area_apresent_km2 is present in inter (it usually is because first input was presentation)
    if "area_apresent_km2" not in inter.columns:
        inter = inter.merge(gdf_pres_p[[id_field, "area_apresent_km2"]], on=id_field, how="left")

    # convert each target to numeric floats
    for t in targets:
        inter[t] = pd.to_numeric(inter[t], errors="coerce").astype(float)

    # for each aggregation compute values
    for a in aggs:
        if a == "mean":
            # compute weighted contributions for each target and sum per presentation unit
            for t in targets:
                inter[f"_wt_{t}"] = inter[t].fillna(0.0) * (inter["area_inter_km2"] / inter["area_apresent_km2"])
            agg_cols = {f"_wt_{t}": "sum" for t in targets}
            grouped = inter.groupby(id_field, as_index=False).agg(agg_cols)
            # rename to <target>_mean
            grouped = grouped.rename(columns={f"_wt_{t}": f"{t}_mean" for t in targets})
            result_pres = result_pres.merge(grouped, on=id_field, how="left")
        elif a == "median":
            records = []
            for key, grp in inter.groupby(id_field):
                rec = {id_field: key}
                for t in targets:
                    vals = grp[t].to_numpy(dtype=float)
                    w = grp["area_inter_km2"].to_numpy(dtype=float)
                    rec[f"{t}_median"] = _weighted_median(vals, w)
                records.append(rec)
            if records:
                agg_med = pd.DataFrame(records)
                result_pres = result_pres.merge(agg_med, on=id_field, how="left")
            else:
                for t in targets:
                    result_pres[f"{t}_median"] = np.nan
        elif a == "max":
            agg_max = inter.groupby(id_field, as_index=False).agg({t: "max" for t in targets})
            agg_max = agg_max.rename(columns={t: f"{t}_max" for t in targets})
            result_pres = result_pres.merge(agg_max, on=id_field, how="left")
        elif a == "min":
            agg_min = inter.groupby(id_field, as_index=False).agg({t: "min" for t in targets})
            agg_min = agg_min.rename(columns={t: f"{t}_min" for t in targets})
            result_pres = result_pres.merge(agg_min, on=id_field, how="left")
        else:
            raise ValueError("Unsupported aggregation: " + str(a))

    # ensure all requested columns exist (fill missing with NaN)
    for t in targets:
        for a in aggs:
            col = f"{t}_{a}"
            if col not in result_pres.columns:
                result_pres[col] = np.nan

    # drop helper columns if any accidentally present
    helper_cols = [c for c in result_pres.columns if c.startswith("_wt_")]
    if helper_cols:
        result_pres = result_pres.drop(columns=helper_cols, errors="ignore")

    # reproject back to presentation CRS and write
    result_pres_out = result_pres.to_crs(gdf_pres.crs)

    if verbose:
        print("Writing aggregated layer", out_layer_name, "to", output_gpkg)
    _safe_write_layer_to_gpkg(output_gpkg, out_layer_name, result_pres_out)

    if verbose:
        print("Aggregation complete. Output GPKG:", output_gpkg, "layer:", out_layer_name)
    return output_gpkg

def cli():
    parser = argparse.ArgumentParser(description="Aggregate cs_ish (and other targets) from ottobacias to a presentation layer.")
    parser.add_argument("cenario", help="Cenário (nome) — the script will look for ./cnr_<cenario>/output/ish_cnr_<cenario>.gpkg by default")
    parser.add_argument("presentation_gpkg", help="Path to presentation gpkg (e.g. apresent_municipios.gpkg)")
    parser.add_argument("--presentation-layer", default=None, help="Layer name inside presentation gpkg (defaults to first layer)")
    parser.add_argument("--id-field", default="id_apresent", help="Identifier field in presentation layer (default: id_apresent)")
    parser.add_argument("--input-gpkg", default=None, help="Input gpkg path (default: ./cnr_<cenario>/output/ish_cnr_<cenario>.gpkg)")
    parser.add_argument("--input-layer", default="regiao_completa", help="Input layer (default: regiao_completa)")
    parser.add_argument("--agg", nargs="+", default=["mean"], help="Aggregation types: mean median max min OR 'all' to compute all. Example: --agg mean median")
    parser.add_argument("--targets", nargs="+", default=["cs_ish"], help="Target columns to aggregate (default cs_ish). Use 'all' to aggregate cs_ish + all columns starting with 'ire_cs_'.")
    args = parser.parse_args()

    if args.input_gpkg is None:
        root = os.getcwd()
        args.input_gpkg = os.path.join(root, f"cnr_{args.cenario}", "output", f"ish_cnr_{args.cenario}.gpkg")

    # normalize comma-separated forms
    if len(args.agg) == 1 and "," in args.agg[0]:
        args.agg = [s.strip() for s in args.agg[0].split(",") if s.strip()]
    if len(args.targets) == 1 and "," in args.targets[0]:
        args.targets = [s.strip() for s in args.targets[0].split(",") if s.strip()]

    output_folder = os.path.join(os.getcwd(), f"cnr_{args.cenario}", "output")
    os.makedirs(output_folder, exist_ok=True)
    output_gpkg = args.input_gpkg

    aggregate_presentation_gpkg(
        input_gpkg=args.input_gpkg,
        input_layer=args.input_layer,
        presentation_gpkg=args.presentation_gpkg,
        presentation_layer=args.presentation_layer,
        id_field=args.id_field,
        aggs=args.agg,
        targets=args.targets,
        output_gpkg=output_gpkg,
        verbose=True
    )

if __name__ == "__main__":
    cli()
